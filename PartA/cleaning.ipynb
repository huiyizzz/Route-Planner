{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open_code\n",
    "from os import path\n",
    "from numpy.core.numeric import NaN\n",
    "from numpy.lib.type_check import _imag_dispatcher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.core.base import DataError\n",
    "from qwikidata.entity import WikidataItem\n",
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "\n",
    "import googlemaps\n",
    "import warnings\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "global unique_name \n",
    "unique_name= set()\n",
    "API_KEY = 'AIzaSyC3O-BdZRrBuOmC_nCvWcbnCdmxEWTztLg'\n",
    "gmaps = googlemaps.Client(key=API_KEY)\n",
    "global stop_words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing data\n",
    "def fill(X):\n",
    "    name = X['name']\n",
    "    if name is NaN:\n",
    "        if 'brand:wikidata' in X['tags']:\n",
    "            wikidata = X['tags']['brand:wikidata']\n",
    "            q_dict = get_entity_dict_from_api(wikidata)\n",
    "            name = WikidataItem(q_dict).get_label()\n",
    "            # print(X.loc[i, ['name', 'tags']])\n",
    "        elif 'brand:wikipedia' in X['tags']:\n",
    "            wikipedia = X['tags']['brand:wikipedia']\n",
    "            name = wikipedia[3:]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find attraction reviews\n",
    "def find_reivew(name):\n",
    "    if name not in unique_name:\n",
    "        unique_name.add(name)\n",
    "        places_result = gmaps.find_place(\n",
    "            input=name, input_type='textquery', fields=['place_id'])\n",
    "        if len(places_result['candidates']) != 0:\n",
    "            place_id = places_result['candidates'][0]['place_id']\n",
    "            place_details = gmaps.place(\n",
    "                place_id=place_id, fields=['name', 'user_ratings_total', 'rating', 'review'])\n",
    "            return place_details['result']\n",
    "\n",
    "\n",
    "def find_review_id(place):\n",
    "    if place['name'] not in unique_name:\n",
    "        unique_name.add(place['name'])\n",
    "        place_details = gmaps.place(\n",
    "            place_id=place['place_id'], fields=['name', 'user_ratings_total', 'rating', 'review'])\n",
    "        return place_details['result']\n",
    "\n",
    "# cleaning name\n",
    "def extract(name):\n",
    "    name = str(name)\n",
    "    if '-' in name:\n",
    "        index = name.index('-')\n",
    "        name = name[:index]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df that contains reviews\n",
    "def create_df(result):\n",
    "\n",
    "    lat = [place[\"geometry\"]['location']['lat'] for place in result['results']]\n",
    "    lon = [place[\"geometry\"]['location']['lng'] for place in result['results']]\n",
    "    name = [place['name'] for place in result['results']]\n",
    "    id = [place['place_id'] for place in result['results']]\n",
    "    data = pd.DataFrame(\n",
    "        data={'name': name, 'place_id': id, 'lat': lat, 'lon': lon})\n",
    "\n",
    "    data = data.loc[(data['lat'] > 49) & (\n",
    "        data['lat'] < 49.5)]\n",
    "    data = data.loc[(data['lon'] > -123.5) & (\n",
    "        data['lon'] < -122)]\n",
    "    review = data.apply(find_review_id, axis=1)\n",
    "    return review\n",
    "\n",
    "# add additional attraction\n",
    "def addition():\n",
    "\n",
    "    result = gmaps.places_nearby(location='49.252336254279, -123.1142930446478',\n",
    "                                 radius=50000, open_now=False, type='tourist_attraction')\n",
    "    review = create_df(result)\n",
    "    review.to_json('./Data/addition.json')\n",
    "    i = 1\n",
    "    while 'next_page_token' in result:\n",
    "        next_token = result['next_page_token']\n",
    "        time.sleep(3)\n",
    "        result = (gmaps.places_nearby(page_token=next_token))\n",
    "        review = create_df(result)\n",
    "        review.to_json('./Data/addition'+str(i)+'.json')\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from CMPT459 Ass1\n",
    "# cleaning review text\n",
    "# --------------------------------------------\n",
    "\n",
    "def remove_url_punctuation(X):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    replace_url = url_pattern.sub(r'', str(X))\n",
    "    punct_pattern = re.compile(r'[^\\w\\s]')\n",
    "    no_punct = punct_pattern.sub(r'', replace_url).lower()\n",
    "    return no_punct\n",
    "\n",
    "\n",
    "def split_words(X):\n",
    "    split_words_list = X.split(' ')\n",
    "    return split_words_list\n",
    "\n",
    "\n",
    "def remove_stopwords(X):\n",
    "    global stop_words\n",
    "    words = []\n",
    "    for word in X:\n",
    "        if word not in stop_words and len(word) > 2 and word != 'nan':\n",
    "            words.append(word)\n",
    "    return words\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "def processing_text(commend):\n",
    "    res = []\n",
    "    for text in commend:\n",
    "        text = text['text']\n",
    "        text = remove_url_punctuation(text)\n",
    "        text = split_words(text)\n",
    "        text = remove_stopwords(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = [lemmatizer.lemmatize(t) for t in text]\n",
    "        text = [lemmatizer.lemmatize(t, 'v') for t in text]\n",
    "        res.append({'word': text})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check spelling\n",
    "def spelling(reviews):\n",
    "    res = []\n",
    "    for commend in reviews:\n",
    "        if commend['language'] == 'en':\n",
    "            text = commend['text']\n",
    "            temp = TextBlob(text)\n",
    "            text = temp.correct()\n",
    "            res.append({'text': str(text)})\n",
    "    return res\n",
    "\n",
    "# find attraction location\n",
    "def find_location(row):\n",
    "    loc = gmaps.find_place(input=row['name'], input_type='textquery',\n",
    "                           fields=['geometry/location'])\n",
    "    loc = loc['candidates'][0]['geometry']\n",
    "    row['lat'] = loc['location']['lat']\n",
    "    row['lon'] = loc['location']['lng']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(out_directory):\n",
    "\n",
    "    # read files and fill missing information\n",
    "    file = './Data/osm/amenities-vancouver.json.gz'\n",
    "    osm_df = pd.read_json(file, lines=True)\n",
    "    osm_df['name'] = osm_df.apply(fill, axis=1)\n",
    "\n",
    "    # select useful amenity\n",
    "    attraction = osm_df.loc[(osm_df['amenity'] == 'clock')\n",
    "                            | (osm_df['amenity'] == 'bicycle_rental') | (osm_df['amenity'] == 'arts_centre') | (osm_df['amenity'] == 'park') | (osm_df['amenity'] == 'nightclub')]\n",
    "\n",
    "    attraction['name'] = attraction['name'].apply(extract)\n",
    "\n",
    "    # find visiters' commends\n",
    "    if not (path.exists('./Data/reviews.json')):\n",
    "        reviews = attraction['name'].apply(find_reivew)\n",
    "        reviews.to_json('./Data/reviews.json')\n",
    "\n",
    "    # add more attraction in Vancouver\n",
    "    if not(path.exists('./Data/addition.json')):\n",
    "        addition()\n",
    "\n",
    "    # combine comments df\n",
    "    reviews = (pd.read_json('./Data/reviews.json', orient='records')).T\n",
    "    add1 = (pd.read_json('./Data/addition.json', orient='records')).T\n",
    "    add2 = (pd.read_json('./Data/addition1.json', orient='records')).T\n",
    "    add3 = (pd.read_json('./Data/addition2.json', orient='records')).T\n",
    "    reviews = pd.concat([reviews, add1, add2, add3])\n",
    "\n",
    "    # clean data\n",
    "    reviews = (reviews.reset_index()).drop(columns=['index'])\n",
    "    reviews = reviews.dropna()\n",
    "    reviews = reviews.loc[reviews['user_ratings_total'] > 1000]\n",
    "\n",
    "    # cleaning commends\n",
    "    reviews['clean_text'] = reviews['reviews'].apply(spelling)\n",
    "    reviews = reviews.loc[reviews['clean_text'] != '']\n",
    "    reviews['word_list'] = reviews['clean_text'].apply(processing_text)\n",
    "    reviews = reviews.loc[reviews['word_list'] != '']\n",
    "    reviews = reviews.drop_duplicates(subset=['name'])\n",
    "    \n",
    "    # select attraction with rank >4.5\n",
    "    reviews = reviews.loc[reviews['rating'] > 4.5]\n",
    "    reviews = reviews.apply(find_location, axis=1)\n",
    "    \n",
    "    # save and use for analyzing.py\n",
    "    reviews.to_json(out_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    out_directory = './Data/data.json'\n",
    "    main(out_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
